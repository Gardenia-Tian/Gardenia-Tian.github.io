{"title":"Pytorch多Stream管理","uid":"a42a03a50c6532bb0083a5ae2b8230f6","slug":"Pytorch多Stream管理","date":"2023-02-28T12:59:48.000Z","updated":"2023-02-28T13:26:02.671Z","comments":true,"path":"api/articles/Pytorch多Stream管理.json","keywords":null,"cover":null,"content":"<p>import shutil</p>\n<p>import torch</p>\n<p>import torch.nn as nn</p>\n<p>import torch.nn.functional as F</p>\n<p>class ModelBaseline(nn.Module):</p>\n<p>​    def <strong>init</strong>(self, d, n=4):</p>\n<p>​        super().<strong>init</strong>()</p>\n<p>​        self.norm = nn.LayerNorm(d)</p>\n<p>​        self.linear1 = nn.Linear(d, d * n)</p>\n<p>​        self.linear2 = nn.Linear(d * 4, d)</p>\n<p>​        self.offload_stream = torch.cuda.Stream()</p>\n<p>​    def forward(self, src: torch.Tensor):</p>\n<p>​        x = self.norm(src)</p>\n<p>​        src = <a href=\"http://src.to\">src.to</a>(device=‘cpu’, non_blocking=True)</p>\n<p>​        x = self.linear2(F.relu(self.linear1(x)))</p>\n<p>​        src = <a href=\"http://src.to\">src.to</a>(device=‘cuda’, non_blocking=True)</p>\n<p>​        return src + x</p>\n<p>class ModelWaitStream(nn.Module):</p>\n<p>​    def <strong>init</strong>(self, d, n=4):</p>\n<p>​        super().<strong>init</strong>()</p>\n<p>​        self.norm = nn.LayerNorm(d)</p>\n<p>​        self.linear1 = nn.Linear(d, d * n)</p>\n<p>​        self.linear2 = nn.Linear(d * 4, d)</p>\n<p>​        self.offload_stream = torch.cuda.Stream()</p>\n<p>​    def forward(self, src: torch.Tensor):</p>\n<p>​        x = self.norm(src)</p>\n<p>​        self.offload_stream.wait_stream(torch.cuda.current_stream())</p>\n<p>​        with torch.cuda.stream(self.offload_stream):</p>\n<p>​            src = <a href=\"http://src.to\">src.to</a>(device=‘cpu’, non_blocking=True)</p>\n<p>​        x = self.linear2(F.relu(self.linear1(x)))</p>\n<p>​        with torch.cuda.stream(self.offload_stream):</p>\n<p>​            src = <a href=\"http://src.to\">src.to</a>(device=‘cuda’, non_blocking=True)</p>\n<p>​            evt_2 = self.offload_stream.record_event()</p>\n<p>​        torch.cuda.current_stream().wait_stream(self.offload_stream)</p>\n<p>​        return src + x</p>\n<p>class ModelWaitEvent(nn.Module):</p>\n<p>​    def <strong>init</strong>(self, d, n=4):</p>\n<p>​        super().<strong>init</strong>()</p>\n<p>​        self.norm = nn.LayerNorm(d)</p>\n<p>​        self.linear1 = nn.Linear(d, d * n)</p>\n<p>​        self.linear2 = nn.Linear(d * 4, d)</p>\n<p>​        self.offload_stream = torch.cuda.Stream()</p>\n<p>​    def forward(self, src: torch.Tensor):</p>\n<p>​        x = self.norm(src)</p>\n<p>​        evt_1 = torch.cuda.current_stream().record_event()</p>\n<p>​        with torch.cuda.stream(self.offload_stream):</p>\n<p>​            self.offload_stream.wait_event(evt_1)</p>\n<p>​            src = <a href=\"http://src.to\">src.to</a>(device=‘cpu’, non_blocking=True)</p>\n<p>​        x = self.linear2(F.relu(self.linear1(x)))</p>\n<p>​        with torch.cuda.stream(self.offload_stream):</p>\n<p>​            src = <a href=\"http://src.to\">src.to</a>(device=‘cuda’, non_blocking=True)</p>\n<p>​            evt_2 = self.offload_stream.record_event()</p>\n<p>​        torch.cuda.current_stream().wait_event(evt_2)</p>\n<p>​        return src + x</p>\n","feature":true,"text":"import shutil import torch import torch.nn as nn import torch.nn.functional as F class ModelBaseline(nn.Module): ​ def init(self, d, n=4): ​...","link":"","photos":[],"count_time":{"symbolsCount":"2.3k","symbolsTime":"2 mins."},"categories":[],"tags":[],"toc":"","author":{"name":"Gardenia","slug":"blog-author","avatar":"/features/profile.jpg","link":"/","description":"People love what other people are passionate about.","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"prev_post":{"title":"源码编译pytorch踩坑记录","uid":"1d5abbd0f9937fba3b7aa81363ed1eab","slug":"源码编译pytorch踩坑记录","date":"2023-03-06T01:28:00.000Z","updated":"2023-03-08T08:38:07.539Z","comments":true,"path":"api/articles/源码编译pytorch踩坑记录.json","keywords":null,"cover":"https://github.com/pytorch/pytorch/blob/master/docs/source/_static/img/pytorch-logo-dark.png","text":"源码编译Pytorch踩坑记录 获取pytorch源码 conda install mkl mkl-include # CUDA only: Add LAPACK support for the GPU if needed conda install -c pytorch mag...","link":"","photos":[],"count_time":{"symbolsCount":"1.7k","symbolsTime":"2 mins."},"categories":[{"name":"清浅集","slug":"清浅集","count":3,"path":"api/categories/清浅集.json"}],"tags":[{"name":"pytorch","slug":"pytorch","count":1,"path":"api/tags/pytorch.json"}],"author":{"name":"Gardenia","slug":"blog-author","avatar":"/features/profile.jpg","link":"/","description":"People love what other people are passionate about.","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true},"next_post":{"title":"C++的STL容器","uid":"8cc29657b5b51898c34ba4535ceced8c","slug":"C-的STL容器","date":"2023-02-24T01:17:03.000Z","updated":"2023-03-10T08:54:00.424Z","comments":true,"path":"api/articles/C-的STL容器.json","keywords":null,"cover":[],"text":"C++的STL容器 本文参考自《C语言中文网》中的STL部分，侵删。 C++的标准模板库(STL)的容器主要可以分为序列式容器, 关联式容器, 无序关联式容器. 序列式容器: 包括 array、vector、deque、list 和 forward_list 容器。 所谓STL序...","link":"","photos":[],"count_time":{"symbolsCount":"20k","symbolsTime":"18 mins."},"categories":[{"name":"清浅集","slug":"清浅集","count":3,"path":"api/categories/清浅集.json"}],"tags":[{"name":"cpp","slug":"cpp","count":1,"path":"api/tags/cpp.json"},{"name":"stl","slug":"stl","count":1,"path":"api/tags/stl.json"},{"name":"编程","slug":"编程","count":1,"path":"api/tags/编程.json"}],"author":{"name":"Gardenia","slug":"blog-author","avatar":"/features/profile.jpg","link":"/","description":"People love what other people are passionate about.","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true}}